from __future__ import annotations

import os
LLM_DISABLED = os.getenv("LLM_DISABLED", "1") == "1"  # 1 = OFF by default

import asyncio
import logging
import json
from typing import Sequence, List, Dict

from app.config import settings

logger = logging.getLogger(__name__)

# Try to import OpenAI
try:
    from openai import AsyncOpenAI
    from openai.types.chat import ChatCompletionMessageParam
    _client = AsyncOpenAI(api_key=settings.openai_api_key)
    OPENAI_AVAILABLE = True
except ImportError:
    _client = None
    OPENAI_AVAILABLE = False
    logger.warning("OpenAI package not installed. LLM features will not work.")

SYSTEM_BASE = (
    "–¢—ã ‚Äî –∏–Ω–∂–µ–Ω–µ—Ä-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –ø—Ä–æ–µ–∫—Ç—É. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç—Ä–æ–≥–æ –∫–∞–∫ —Ñ–∞–∫—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞. "
    "–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –Ω—É–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º, –∞ –∑–∞—Ç–µ–º –æ—Ç–≤–µ—Ç—å –æ–±—â–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏, "
    "–Ω–µ –≤—ã–¥—É–º—ã–≤–∞—è –ø—Ä–æ–µ–∫—Ç–Ω—ã–µ –¥–µ—Ç–∞–ª–∏. –û—Ç–≤–µ—á–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–æ. –ù–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–π —Ö–æ–¥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."
)

def _make_messages(prompt: str, ctx_chunks: Sequence[str]) -> list[ChatCompletionMessageParam]:
    ctx_block = ""
    if ctx_chunks:
        # –ñ—ë—Å—Ç–∫–æ –æ—Ç–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –Ω–µ ¬´–º–µ—à–∞–ª–∞¬ª –µ–≥–æ —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
        joined = "\n\n---\n".join(ctx_chunks)
        ctx_block = f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã):\n---\n{joined}\n---"
    system = SYSTEM_BASE + ctx_block
    return [
        {"role": "system", "content": system},
        {"role": "user", "content": prompt},
    ]

async def ask_llm(prompt: str, ctx_chunks: Sequence[str], model: str = "gpt-4o", max_tokens: int = 1200) -> str:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç. model: 'gpt-4o' | 'gpt-4o-mini'
    """
    if LLM_DISABLED:
        ctx_n = len(ctx_chunks or [])
        return f"üß™ TEST: LLM –æ—Ç–∫–ª—é—á—ë–Ω.\n–í–æ–ø—Ä–æ—Å: {prompt[:400]}\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {ctx_n} —Ñ—Ä–∞–≥–º."
    
    if not OPENAI_AVAILABLE or _client is None:
        return "‚ö†Ô∏è OpenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ openai>=1.40.0"
    
    messages = _make_messages(prompt, ctx_chunks)
    use_model = "gpt-4o-mini" if model == "gpt-4o-mini" else "gpt-4o"

    try:
        # Chat Completions ‚Äî –Ω–∞–¥—ë–∂–Ω–æ –∏ –ø—Ä–æ—Å—Ç–æ.
        resp = await _client.chat.completions.create(
            model=use_model,
            messages=messages,
            temperature=0.3,
            max_tokens=max_tokens,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        logger.exception("LLM error: %s", e)
        return "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å –∫–ª—é—á/–ª–∏–º–∏—Ç—ã."

async def summarize_text(text: str, model: str | None = None, max_tokens: int = 500) -> str:
    """
    –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –æ—Ç–≤–µ—Ç–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–Ω–æ–ø–∫–æ–π üìå Summary).
    """
    if LLM_DISABLED:
        return "üß™ TEST: LLM –æ—Ç–∫–ª—é—á—ë–Ω. –†–µ–∑—é–º–µ –Ω–µ —Å–æ–∑–¥–∞–Ω–æ."
        
    if not OPENAI_AVAILABLE or _client is None:
        return "‚ö†Ô∏è OpenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ openai>=1.40.0"
        
    model = model or "gpt-4o"
    prompt = (
        "–°–¥–µ–ª–∞–π —Å–∂–∞—Ç–æ–µ, —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–Ω–æ–µ —Ä–µ–∑—é–º–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∏–∂–µ: 5‚Äì10 –ø—É–Ω–∫—Ç–æ–≤ –∏–ª–∏ ~120‚Äì200 —Å–ª–æ–≤. "
        "–ë–µ–∑ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Ç–æ–ª—å–∫–æ –∏—Ç–æ–≥.\n\n–¢–µ–∫—Å—Ç:\n" + text
    )
    messages: list[ChatCompletionMessageParam] = [
        {"role": "system", "content": SYSTEM_BASE},
        {"role": "user", "content": prompt}
    ]
    try:
        resp = await _client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.2,
            max_tokens=max_tokens,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        logger.exception("Summarize error: %s", e)
        return "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–¥–µ–ª–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ."

async def generate_zip_files(task_description: str, context_chunks: Sequence[str], tags: List[str]) -> Dict[str, str]:
    """
    Generate multiple files for ZIP archive based on task description.
    
    Args:
        task_description: Description of what to generate
        context_chunks: Project context
        tags: Tags for filtering/guidance
        
    Returns:
        Dictionary mapping file paths to their content
    """
    if LLM_DISABLED:
        return {"test.txt": "üß™ TEST: LLM –æ—Ç–∫–ª—é—á—ë–Ω. –§–∞–π–ª—ã –Ω–µ —Å–æ–∑–¥–∞–Ω—ã."}
        
    if not OPENAI_AVAILABLE or _client is None:
        return {"error.txt": "OpenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ openai>=1.40.0"}
    
    # Prepare context and prompt
    context = "\n\n".join(context_chunks[:30])
    tags_str = ", ".join(tags) if tags else "general"
    
    prompt = f"""
–í—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫. –°–æ–∑–¥–∞–π—Ç–µ –∞—Ä—Ö–∏–≤ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–¥–∞—á–∏.

–ö–û–ù–¢–ï–ö–°–¢ –ü–†–û–ï–ö–¢–ê:
{context}

–ó–ê–î–ê–ß–ê: {task_description}
–¢–ï–ì–ò: {tags_str}

–ò–ù–°–¢–†–£–ö–¶–ò–ï:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞
2. –°–æ–∑–¥–∞–π—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏
3. –í–µ—Ä–Ω–∏—Ç–µ JSON –≤ —Ñ–æ—Ä–º–∞—Ç–µ: {{"files": {{"path/file.ext": "content", ...}}, "notes": "–æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π"}}
4. –ü—É—Ç–∏ —Ñ–∞–π–ª–æ–≤ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º–∏
5. –í–∫–ª—é—á–∏—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã
6. –î–æ–±–∞–≤—å—Ç–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ –∫–æ–¥

–û–¢–í–ï–¢ (JSON):
"""
    messages: list[ChatCompletionMessageParam] = [
        {"role": "system", "content": SYSTEM_BASE},
        {"role": "user", "content": prompt}
    ]

    try:
        # Call OpenAI API
        resp = await _client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.3,
            max_tokens=2000,
        )
        
        # Parse the response
        content = (resp.choices[0].message.content or "").strip()
        
        # Try to parse as JSON
        try:
            result = json.loads(content)
            return result.get("files", {})
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON from LLM response")
            return {"error.txt": f"Failed to parse response: {content}"}
            
    except Exception as e:
        logger.exception("generate_zip_files error: %s", e)
        return {"error.txt": f"Error generating files: {str(e)}"}

async def generate_single_file(file_path: str, task_description: str, context_chunks: Sequence[str]) -> str:
    """
    Generate content for a single file.
    
    Args:
        file_path: Target file path
        task_description: What to generate
        context_chunks: Project context
        
    Returns:
        File content as string
    """
    if LLM_DISABLED:
        return "# üß™ TEST: LLM –æ—Ç–∫–ª—é—á—ë–Ω. –§–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω."
        
    if not OPENAI_AVAILABLE or _client is None:
        return "# OpenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ openai>=1.40.0"
        
    context = "\n\n".join(context_chunks[:30])
    
    prompt = f"""
–í—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫. –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏.

–ö–û–ù–¢–ï–ö–°–¢ –ü–†–û–ï–ö–¢–ê:
{context}

–§–ê–ô–õ: {file_path}
–ó–ê–î–ê–ß–ê: {task_description}

–ò–ù–°–¢–†–£–ö–¶–ò–ï:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞
2. –°–æ–∑–¥–∞–π—Ç–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏
3. –£—á—Ç–∏—Ç–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ —è–∑—ã–∫–∞/—Ñ–æ—Ä–º–∞—Ç–∞
4. –î–æ–±–∞–≤—å—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
5. –°–ª–µ–¥—É–π—Ç–µ –ª—É—á—à–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º

–°–û–î–ï–†–ñ–ò–ú–û–ï –§–ê–ô–õ–ê:
"""
    messages: list[ChatCompletionMessageParam] = [
        {"role": "system", "content": SYSTEM_BASE},
        {"role": "user", "content": prompt}
    ]

    try:
        # Call OpenAI API
        resp = await _client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.3,
            max_tokens=1500,
        )
        
        return (resp.choices[0].message.content or "").strip()
        
    except Exception as e:
        logger.exception("generate_single_file error: %s", e)
        return f"# Error generating content: {str(e)}"

async def analyze_diff_context(summary: str, context_chunks: Sequence[str]) -> str:
    """
    Analyze diff summary with project context to provide insights.
    
    Args:
        summary: Diff summary
        context_chunks: Project context
        
    Returns:
        Analysis and recommendations
    """
    if LLM_DISABLED:
        return "üß™ TEST: LLM –æ—Ç–∫–ª—é—á—ë–Ω. –ê–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω."
        
    if not OPENAI_AVAILABLE or _client is None:
        return "‚ö†Ô∏è OpenAI SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ openai>=1.40.0"
        
    context = "\n\n".join(context_chunks[:20])
    
    prompt = f"""
–í—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –ø—Ä–æ–µ–∫—Ç–µ.

–ö–û–ù–¢–ï–ö–°–¢ –ü–†–û–ï–ö–¢–ê:
{context}

–ò–ó–ú–ï–ù–ï–ù–ò–Ø:
{summary}

–ò–ù–°–¢–†–£–ö–¶–ò–ï:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä –∏–∑–º–µ–Ω–µ–Ω–∏–π
2. –û—Ü–µ–Ω–∏—Ç–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –ø—Ä–æ–µ–∫—Ç
3. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
4. –í—ã–¥–µ–ª–∏—Ç–µ –≤–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã

–ê–ù–ê–õ–ò–ó:
"""
    messages: list[ChatCompletionMessageParam] = [
        {"role": "system", "content": SYSTEM_BASE},
        {"role": "user", "content": prompt}
    ]

    try:
        # Call OpenAI API
        resp = await _client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.2,
            max_tokens=1000,
        )
        
        return (resp.choices[0].message.content or "").strip()
        
    except Exception as e:
        logger.exception("analyze_diff_context error: %s", e)
        return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {str(e)}"